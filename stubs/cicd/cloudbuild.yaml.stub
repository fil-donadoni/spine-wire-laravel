steps:
  # Step 1: Build and push Docker image to Artifact Registry
  # Uses registry-based caching for faster incremental builds
  - id: "build-and-push"
    name: "gcr.io/cloud-builders/docker"
    entrypoint: bash
    args:
      - '-c'
      - |
        # Build and push Docker image with layer caching
        docker buildx build \
          --push \
          --cache-from type=registry,ref=$_REGION-docker.pkg.dev/$_PROJECT/$_CLIENT-$_APP-$_ENVIRONMENT/$_SERVICE:cache \
          --cache-to type=registry,ref=$_REGION-docker.pkg.dev/$_PROJECT/$_CLIENT-$_APP-$_ENVIRONMENT/$_SERVICE:cache,mode=max \
          --file docker/Dockerfile \
          -t "$_REGION-docker.pkg.dev/$_PROJECT/$_CLIENT-$_APP-$_ENVIRONMENT/$_SERVICE:latest" \
          .

  # Step 2: Update Cloud Run Service with new image
  # Only updates the image, all other configuration is managed by Terraform
  # Runs in parallel with update_jobs after build completes
  - id: "update_service"
    name: "gcr.io/google.com/cloudsdktool/cloud-sdk:alpine"
    waitFor: ["build-and-push"]
    entrypoint: gcloud
    args:
      - "run"
      - "services"
      - "update"
      - "$_CLIENT-$_SERVICE-$_ENVIRONMENT"
      - "--image=$_REGION-docker.pkg.dev/$_PROJECT/$_CLIENT-$_APP-$_ENVIRONMENT/$_SERVICE:latest"
      - "--region=$_REGION"
      - "--max-instances=$_MAX_INSTANCES"
      - "--min-instances=$_MIN_INSTANCES"

  # Step 3: Update Cloud Run Jobs with new image (if any exist)
  # Automatically discovers jobs based on labels - no manual configuration needed
  # Jobs are created by Terraform with placeholder image, this step updates them
  # Runs in parallel with update_service after build completes
  - id: "update_jobs"
    name: "gcr.io/google.com/cloudsdktool/cloud-sdk:alpine"
    waitFor: ["build-and-push"]
    entrypoint: bash
    args:
      - '-c'
      - |
        set -e

        IMAGE="$_REGION-docker.pkg.dev/$_PROJECT/$_CLIENT-$_APP-$_ENVIRONMENT/$_SERVICE:latest"

        echo "=================================================="
        echo "Checking for Cloud Run Jobs to update..."
        echo "Region: $_REGION"
        echo "Client: $_CLIENT"
        echo "Environment: $_ENVIRONMENT"
        echo "Image: $${IMAGE}"
        echo "=================================================="

        # Find all jobs for this client/environment using labels
        # This automatically picks up any jobs created by Terraform
        jobs=$(gcloud run jobs list \
          --region="$_REGION" \
          --filter="metadata.labels.client=$_CLIENT AND metadata.labels.environment=$_ENVIRONMENT" \
          --format="value(metadata.name)" 2>/dev/null || echo "")

        if [ -z "$${jobs}" ]; then
          echo "‚ÑπÔ∏è  No Cloud Run Jobs found for this environment"
          echo "   (This is normal if scheduled_jobs feature is not enabled)"
          exit 0
        fi

        job_count=$(echo "$${jobs}" | wc -l | tr -d ' ')
        echo "Found $${job_count} jobs to update:"
        echo "$${jobs}" | sed 's/^/  - /'
        echo ""

        # Update all jobs in parallel (up to 10 concurrent)
        echo "$${jobs}" | xargs -P 10 -I {} sh -c '
          echo "üîÑ Updating job: {}"
          gcloud run jobs update "{}" \
            --image="'"$${IMAGE}"'" \
            --region="$_REGION" \
            --quiet
          echo "‚úÖ Updated: {}"
        '

        echo ""
        echo "=================================================="
        echo "‚úÖ Successfully updated $${job_count} Cloud Run Job(s)"
        echo "=================================================="

# Build options
options:
  logging: CLOUD_LOGGING_ONLY
  # Use faster machine for quicker builds (default: E2_MEDIUM)
  # Options: E2_MEDIUM, E2_HIGHCPU_8, E2_HIGHCPU_32
  machineType: E2_HIGHCPU_8
